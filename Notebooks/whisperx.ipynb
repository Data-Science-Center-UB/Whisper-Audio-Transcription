{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767a11f6-a1ba-4a58-b95e-b5576454ac45",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deac193-d106-464c-abbe-4b3261c1c00d",
   "metadata": {},
   "source": [
    "This notebook applies [WhisperX](https://github.com/m-bain/whisperX) with installation procedures based on the official GitHub repository (accessed July 29, 2025). See also the faster-whisper-pyannote notebook for additional general information on setup and for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93abb6-6403-4ef1-b219-70875d6a1958",
   "metadata": {},
   "source": [
    "# 1. One-Time Setup: Install Software & Hugging Face Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703f3b2-d771-4670-9746-56502c5a4fbb",
   "metadata": {},
   "source": [
    "## 1.1 FFmpeg\n",
    "\n",
    "WhisperX uses FFmpeg to load and preprocess audio (e.g., convert .mp3 to .wav, resample to 16kHz mono). It is required even if the input audio is already in .wav format, because WhisperX always calls FFmpeg internally.\n",
    "\n",
    "> 16kHz mono? The audio is stored with one channel (not left/right stereo) and sampled 16,000 times per second.\n",
    "\n",
    "FFmpeg can be installed in Jupyter with conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1c2e8-0b27-4db6-8fad-a697e277ed6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46333a3-5151-47aa-b5df-835919f13a2b",
   "metadata": {},
   "source": [
    "Alternatively, FFmpeg can be installed system-wide, and added to the environment PATH of your system to make it accessible from anywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604421c-5db7-41f7-bf51-8dd1b5cdc403",
   "metadata": {},
   "source": [
    "Once installed, test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cbdb0-3d73-4e0f-8881-a4a7e047b26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de50fb-e1c6-40ca-b1a8-b6530f92a5a6",
   "metadata": {},
   "source": [
    "## 1.2 WhisperX\n",
    "\n",
    "The easiest way to install WhisperX, according to its GitHub page, is via PyPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d86ac-a895-4bfe-ac79-8a82ee845c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install whisperx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8c079-5ddf-4dc8-bbf3-b9dfc82d38a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"pyannote.audio==3.4.0\" # previous pyannote.audio version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5599650-7fb6-4a35-b7a3-9514e196a388",
   "metadata": {},
   "source": [
    "This installs WhisperX along with its core dependencies, including pyannote.audio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ddefe-40c0-46c6-a98b-7cb8bf697a4f",
   "metadata": {},
   "source": [
    "## 1.3 Hugging Face Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e914ae-fea9-4c0f-91de-6d8a891c5379",
   "metadata": {},
   "source": [
    "If you don't have a [Hugging Face Account](https://huggingface.co/) you need to create one. \n",
    "\n",
    "You will require token later on for the speaker diarization. You can create one by clicking on your profile icon; next click on \"Access Tokens\": Create a new access token. \n",
    "\n",
    ">Important: Don't share your token.\n",
    "\n",
    "In addition, pyannote requires you to agree to share your contact information to access it's models. For that, go on the [pyannote speaker-diarization model](https://huggingface.co/pyannote/speaker-diarization-3.1) page, enter your information, and click on \"Agree and access repository\". Do the same for the [pyannote segmentation model](https://huggingface.co/pyannote/segmentation-3.0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3a891-b86b-4e45-baa9-5efb6533a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_token = \"ENTER_YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2448a-380a-4d45-9b39-c54de5147286",
   "metadata": {},
   "source": [
    "# 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687f5a3-23e7-4570-a58b-a48215925675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937a92d-d89f-4f07-a1ad-ae35e20c51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx # Whisperx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec50344-4aeb-4efc-9df9-6ff59b22bf74",
   "metadata": {},
   "source": [
    "You can check the installed PyTorch version and whether your environment has access to a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9ef35-9e0d-4d12-b77a-564d56cea5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae09e4-29f0-482d-a43e-94fe930ab862",
   "metadata": {},
   "source": [
    "# 3. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a172c-56ce-4c9d-8be2-d8be7cd4a4e3",
   "metadata": {},
   "source": [
    "## 3.1 Runtime Setup\n",
    "\n",
    "Automatically set device and compute type depending on hardware availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7832e-8820-40b1-a714-cd1d0ee5822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    compute_type = \"float16\"  # Faster and more memory efficient on GPU\n",
    "    batch_size = 16           # Adjust based on GPU memory\n",
    "else:\n",
    "    compute_type = \"int8\"     # Required or more efficient on CPU\n",
    "    batch_size = 1            # Keep at 1 on CPU (larger values donâ€™t help and may cause memory issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29bbcb-85cc-4786-bb12-77792e69a5cb",
   "metadata": {},
   "source": [
    ">Running WhisperX on CPU does not reduce transcription or diarization accuracy, but it significantly slows down processing time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be2294-dc5e-48f9-b1f8-62f9668778d5",
   "metadata": {},
   "source": [
    "## 3.2 Select Audio File\n",
    "\n",
    "Provide the relative path to one audio file (goes up one folder into, e.g., \"Data/buffy/\"). Both .wav and .mp3 files work because the transcription library uses ffmpeg under the hood to read many common audio formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67bd96-a5f7-4c05-9577-97d85aba9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../Data/buffy/shortened_Buffy_Seas01-Epis01.en.wav\"\n",
    "#audio_file = \"../Data/moon-landing/CA138clip.mp3\"\n",
    "#audio_file = \"../Data/qualitative-interview-de/DE_example_2.mp3\"\n",
    "#audio_file = \"../Data/qualitative-interview-en/EN_example_1.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75118277-b7e6-4684-9109-9b8725ce47d4",
   "metadata": {},
   "source": [
    "# 4. Load Whisper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd2289-facb-4858-a7e6-1cab75f9f372",
   "metadata": {},
   "source": [
    "If you know the language, set it explicitly. This reduces errors and makes decoding faster. The \"language\" variable will directly be used by WhisperX during model loading and transcription, so the model knows in advance which language to expect instead of trying to detect it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d81661-0418-4256-b533-4e1d74ae9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480f693-beab-4f32-af72-fc9cb9888e18",
   "metadata": {},
   "source": [
    "Load the model with the given device (\"cpu\" or \"cuda\") and precision type (\"float16\", \"int8\", etc.). Refer to [Whisper](https://github.com/openai/whisper) or use a custom (e.g. fine-tuned) model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db4f85-751a-4356-8025-0220749f78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisperx.load_model(\"tiny\", \n",
    "                            device, \n",
    "                            compute_type=compute_type, \n",
    "                            language=language) # \"tiny\", ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226dd39-2db6-405b-92c6-8d25fe7f427a",
   "metadata": {},
   "source": [
    "# 5. Automatic Speech Recognition (ASR)\n",
    "\n",
    "Transcription happens after Voice Activity Detection (VAD) and segmentation. Segments are grouped into ~30s chunks and passed to original Whisper (potentially in batches for faster transcription on GPU; higher batch size means more parallel processing). For details on available decoding and alignment options, refer to the [WhisperX](https://github.com/m-bain/whisperX) documentation.\n",
    "\n",
    "WhisperX outputs a list of segments, where each segment is a dictionary containing the recognized text along with its start and end times in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa3206-d397-4f79-bd2f-60d7865bbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisperx.load_audio(audio_file)\n",
    "asr_result = model.transcribe(audio, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c1941-661e-48ee-b04a-3757aa48015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asr_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76143f-2785-46f5-935b-3cb9f9bfa5f8",
   "metadata": {},
   "source": [
    "# 6. Word-level Forced Alignment\n",
    "\n",
    "WhisperX realigns the text by matching the audio to phoneme probabilities using DTW (dynamic time warping), which gives more accurate word timings. After whisperx.align(), the result is a dictionary whose segments now include word-level alignments. Each segment still has text, start, and end, and gains a words list where every entry has the word string plus precise start/end times and an alignment score useful for spotting poorly aligned or uncertain timings (this is not a metric for ASR accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ac80b-ef8e-4e5c-b476-f9510ad86eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a, metadata = whisperx.load_align_model(language_code=asr_result[\"language\"], device=device)\n",
    "aligned_result = whisperx.align(asr_result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a8c9d-6a13-4132-9f74-0a92cdfadff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(aligned_result) # Now contains accurate word-level start/end timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aab9e7-b8bd-4451-b176-42b2c772daf7",
   "metadata": {},
   "source": [
    "# 7. Speaker Diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd3e77-4467-4c7c-a05a-7b7094291c01",
   "metadata": {},
   "source": [
    "## 7.1 Load Model\n",
    "\n",
    "Diarization is loaded through WhisperX, which wraps the pyannote model internally instead of calling it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8dcbb6-2e7f-49a7-ad89-14ea43a33985",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarize_model = whisperx.diarize.DiarizationPipeline(use_auth_token=own_token, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f9fc5-a0cf-45e9-a0a0-5976033c4153",
   "metadata": {},
   "source": [
    "## 7.2 Perform Diarization\n",
    "\n",
    "We run the diarization model on the audio file. This gives us a timeline of speaker activity, for example: SPEAKER_00 speaks from 0â€“10 seconds, then SPEAKER_01 speaks from 10â€“15 seconds, and so on. By setting both min and max speakers to 2, we force the model to split the conversation into exactly two different speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335ae46-3184-406d-ae2d-f4a57e04688c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_result = diarize_model(audio,\n",
    "                                   min_speakers=2, \n",
    "                                   max_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cad6a-05a3-4fa2-a372-8906b448745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diarization_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03714ede-d6f3-4e88-a616-f5c73b802da3",
   "metadata": {},
   "source": [
    "# 8. Merge Results\n",
    "\n",
    "We merge ASR output with diarization so that each spoken segment (or each word) is linked to a speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e067f-9d6c-4729-8582-efb2f8a5ff9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_result = whisperx.assign_word_speakers(diarization_result, aligned_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ac940-363e-4941-ba89-2bf75cbec25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_result) # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8ad03-8053-460e-80dc-c63bf79610c6",
   "metadata": {},
   "source": [
    "# 9. Save Final Result\n",
    "\n",
    "Finally, we save a readable transcript as a text file: for each segment, we write out the speaker label, the transcribed text, and the segmentâ€™s start time. This produces a simple \"Speaker: text [time]\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd7d58-77f5-43c8-b8bf-16d1af5ae1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as ...\n",
    "method = \"WhisperX\"\n",
    "file = \"Buffy\"\n",
    "output_folder = \"../Results/\"\n",
    "output_name = file + \"_\" + method\n",
    "txt_path = os.path.join(output_folder, f\"{output_name}.txt\")\n",
    "\n",
    "# Save \n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for seg in final_result[\"segments\"]:\n",
    "        speaker = seg.get(\"speaker\")  # already present in your example\n",
    "        text = seg[\"text\"].strip()\n",
    "        start = str(timedelta(seconds=seg[\"start\"]))[:-3]\n",
    "        f.write(f\"{speaker}: {text} [{start}]\\n\\n\")\n",
    "\n",
    "print(\"Saved transcript to\", txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb680664-c581-48e3-ace5-16d79d15f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
