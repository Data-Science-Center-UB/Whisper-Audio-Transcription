{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64eca1d-8325-4896-a6eb-fb348d953746",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e05430-d65e-4b49-9d77-cf097a38f663",
   "metadata": {},
   "source": [
    "This notebook applies a pipeline that is built on [faster-whisper](https://github.com/SYSTRAN/faster-whisper) and [pyannote.audio](https://github.com/pyannote/pyannote-audio), with installation procedures based on their official GitHub repositories (accessed September 25, 2025). This notebook also includes some comparisons with the [WhisperX](https://github.com/m-bain/whisperX) workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79bc53-5394-4516-82c3-1c7c498eb7c9",
   "metadata": {},
   "source": [
    "# 1. One-Time Setup: Install Software & Hugging Face Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9843fed-7de3-4766-b9e9-a99f51b281b9",
   "metadata": {},
   "source": [
    "## 1.1 FFmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0df04-6048-4ce5-98c2-cc753f6c0a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad2619-4d9b-4d73-9885-fb8fc7b86991",
   "metadata": {},
   "source": [
    "## 1.2 faster_whisper (CTranslate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2839d1e-1034-4717-ae85-052dabfa5746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install faster-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448831b7-6b32-4fc2-99c7-26f6e881aa40",
   "metadata": {},
   "source": [
    "## 1.3 pyannote\n",
    "\n",
    "Compared to WhisperX, the faster-whisper + pyannote workflow requires you to load and combine the diarization model yourself, which makes the process a bit more manual but also more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b9b0c-f54f-49e0-8c5b-53bab34e229d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed07d1-1344-4c9f-b755-577854e35490",
   "metadata": {},
   "source": [
    "## 1.4 Hugging Face Account\n",
    "\n",
    "If you don't have a [Hugging Face Account](https://huggingface.co/) you need to create one. \n",
    "\n",
    "You will require token later on for the speaker diarization. You can create one by clicking on your profile icon; next click on \"Access Tokens\": Create a new access token. \n",
    "\n",
    ">Important: Don't share your token.\n",
    "\n",
    "In addition, pyannote requires you to agree to share your contact information to access it's models. For that, go on the [pyannote speaker-diarization model](https://huggingface.co/pyannote/speaker-diarization-3.1) page, enter your information, and click on \"Agree and access repository\". Do the same for the [pyannote segmentation model](https://huggingface.co/pyannote/segmentation-3.0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb78090-1ec5-44eb-a23a-c43a30ff419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_token = \"ENTER_YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df78052-e7cc-480d-a742-ba2b62ca189f",
   "metadata": {},
   "source": [
    "# 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07700c-022c-4e81-aac2-1695b3e0f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c47f70-2b99-473e-855a-1539652fdc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel # faster-whisper\n",
    "from faster_whisper import BatchedInferencePipeline # if batched transcription is wanted\n",
    "\n",
    "from pyannote.audio import Pipeline # pyannote: a wrapper that gives ready-to-use model for speaker diarization task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d74e8-cee9-459d-8d6e-d88bc839dfe9",
   "metadata": {},
   "source": [
    "You can check the installed PyTorch version and whether your environment has access to a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789cca8-dc80-43b6-9646-db8d6ce158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c899e-4f39-4401-8c74-af81d48de1bc",
   "metadata": {},
   "source": [
    "# 3. Setup\n",
    "\n",
    "## 3.1 Runtime Setup\n",
    "\n",
    "Automatically set device and compute type depending on hardware availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4baa74-0c13-4507-a9ca-cb5efe98c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    compute_type = \"float16\"  # Faster and more memory efficient on GPU\n",
    "    batch_size = 16           # Adjust based on GPU memory\n",
    "else:\n",
    "    compute_type = \"int8\"     # Required or more efficient on CPU\n",
    "    batch_size = 1            # Keep at 1 on CPU (larger values donâ€™t help and may cause memory issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569593c-ede0-4a8a-8020-3d7b55671224",
   "metadata": {},
   "source": [
    "## 3.2 Select Audio File\n",
    "\n",
    "Provide the relative path to one audio file (goes up one folder into, e.g., \"Data/buffy/\"). Both .wav and .mp3 files work because the transcription library uses ffmpeg under the hood to read many common audio formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e63aa-771e-4a33-b8cb-6a4aebdcad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../Data/buffy/shortened_Buffy_Seas01-Epis01.en.wav\"\n",
    "#audio_file = \"../Data/moon-landing/CA138clip.wav\"\n",
    "#audio_file = \"../Data/qualitative-interview-de/DE_example_2.mp3\"\n",
    "#audio_file = \"../Data/qualitative-interview-en/EN_example_1.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263afb6-ca29-4d0e-936e-056a1229f424",
   "metadata": {},
   "source": [
    "# 4. Load Whisper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20a817-1d9f-49f0-9d2f-251bc92f0d77",
   "metadata": {},
   "source": [
    "If you know the language, set it explicitly. This reduces errors and makes decoding faster. The \"language\" variable is passed when calling model.transcribe() in Sect. 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2838a8f-eff8-45d0-ba6a-1d04209a347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083761cc-f876-426d-9533-3063d2d21318",
   "metadata": {},
   "source": [
    "Load the model with the given device (\"cpu\" or \"cuda\") and precision type (\"float16\", \"int8\", etc.). Refer to [Whisper](https://github.com/openai/whisper) or use a custom (e.g. fine-tuned) model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982a27-9bd6-4e96-a0c0-291ae0af4be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = WhisperModel(\"large-v3\", \n",
    "                     device, \n",
    "                     compute_type=compute_type) # \"tiny\", ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316311f-94f3-4fe5-8266-83220fbce9d4",
   "metadata": {},
   "source": [
    "With faster-whisper you can call the model in two ways: with or without batching enabled. Use batched only on GPU. If you want the batched model (alternative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d2651-225c-4744-a96b-4eff3312f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batched_model = BatchedInferencePipeline(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4be45-c508-4e27-bb17-1b808f2d02a1",
   "metadata": {},
   "source": [
    "# 5. Automatic Speech Recognition (ASR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03298dd2-22f6-4259-a5a7-65e0228dbe74",
   "metadata": {},
   "source": [
    "The transcription process can be tuned through several parameters. In the setup below, we enable word-level timestamps and the Voice Activity Detection (VAD) filter. Word-level timestamps are generated during decoding, while the VAD filter runs as a preprocessing step before transcription. Beam size controls how many alternative transcriptions the model considers at each step (larger values can improve accuracy slightly but make decoding slower). For details on available decoding and alignment options, refer to the [faster-whisper](https://github.com/SYSTRAN/faster-whisper) documentation.\n",
    "\n",
    "faster-whisper outputs a list of segments, where each segment corresponds to a decoded audio chunk of up to ~30 seconds. Every segment includes its start and end time, the recognized text, the underlying token IDs, and several confidence-related metrics (avg_logprob, compression_ratio, no_speech_prob) that could be further analyzed. If word-level timestamps are enabled, each segment also contains a list of words with their own start and end times and probabilities.\n",
    "\n",
    ">Comparison with WhisperX: WhisperX always applies two additional models: a speech activity detector to remove silence (VAD) and a forced aligner to re-time each word with high precision. By contrast, with faster-whisper, VAD is just a lightweight filter before decoding, and word timings are produced directly by Whisper. This difference can make WhisperX word timings more precise. WhisperX skips confidence-related metrics because its focus is on producing transcriptions with accurate word-level timings, while faster-whisper exposes additional internal information from Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70298242-ef91-4046-a019-956932ce1ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\n",
    "    audio_file, \n",
    "    beam_size=1, \n",
    "    word_timestamps=True, \n",
    "    vad_filter=True, \n",
    "    language=\"en\") \n",
    "\n",
    "segments = list(segments)  # The transcription will actually run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46051fae-cef1-4e45-a7f3-75063e8a02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434ce89-04ab-434a-9058-ae77214c98f0",
   "metadata": {},
   "source": [
    "Alternative with batched model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81ef76-b3d4-420b-9187-bf3bd498a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segments, info = batched_model.transcribe(audio_file, batch_size=batch_size, word_timestamps=True, vad_filter=True, language=\"en\")\n",
    "\n",
    "#segments = list(segments)  # The transcription will actually run here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a817a8-e86f-4885-9f20-edb7ac658e9e",
   "metadata": {},
   "source": [
    "# 6. Speaker Diarization\n",
    "\n",
    "## 6.1 Load Model\n",
    "\n",
    "First, we load the speaker diarization model from pyannote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a37d2-1d93-40f0-8c4f-b1609f25d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=own_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533ddbf-a512-4e79-a9cf-647e1dd6b958",
   "metadata": {},
   "source": [
    "## 6.2 Perform Diarization\n",
    "\n",
    "Next, we run the diarization model on the audio file. This gives us a timeline of speaker activity, for example: SPEAKER_00 speaks from 0â€“10 seconds, then SPEAKER_01 speaks from 10â€“15 seconds, and so on. By setting both min and max speakers to 2, we force the model to split the conversation into exactly two different speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1af5d-34d0-4e3b-8a82-105ec8dd5980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diarization_result = pipeline(audio_file, \n",
    "                              min_speakers=2, \n",
    "                              max_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c996dce-a3a3-4b42-89ca-a7652610a7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(diarization_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d24bb8-6174-49b6-a12a-b329b89d4880",
   "metadata": {},
   "source": [
    "# 7. Merge Results\n",
    "\n",
    "We merge ASR output with diarization so that each spoken segment (or each word) is linked to a speaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d469faf-3e54-48fb-90d8-faa22c9ff912",
   "metadata": {},
   "source": [
    "## 7.1 ASR Result Formating\n",
    "\n",
    "Since the output of faster-whisper is stored in its own objects, we first convert it into a simple Python dictionary format. We define a function \"to_whisper_result\". This makes the transcript easier to work with. Each segment gets a start time, an end time, the transcribed text, and (if word-level timestamps were enabled) a list of words with their own timing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b8cd3-85b7-4472-ba2a-1fc7bb7c9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_whisper_result(segments, language=None):\n",
    "    out = {\"segments\": []}\n",
    "    for s in segments:\n",
    "        item = {\n",
    "            \"start\": float(s.start),\n",
    "            \"end\": float(s.end),\n",
    "            \"text\": s.text or \"\",\n",
    "        }\n",
    "        if getattr(s, \"words\", None):\n",
    "            item[\"words\"] = [\n",
    "                {\n",
    "                    \"word\": w.word,\n",
    "                    \"start\": float(w.start),\n",
    "                    \"end\": float(w.end),\n",
    "                }\n",
    "                for w in s.words\n",
    "                if w.start is not None and w.end is not None\n",
    "            ]\n",
    "        out[\"segments\"].append(item)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba196bc-7fa4-416e-b04c-15ddd9baa428",
   "metadata": {},
   "source": [
    "Run conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d8395-4a3d-485f-94d3-9ac82f53f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_result = to_whisper_result(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4590e0e-e1ee-43e1-ac54-032c3f48be55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(asr_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf850a-9e20-4308-87ea-64d432fbd16b",
   "metadata": {},
   "source": [
    "## 7.2 Align Results\n",
    "\n",
    "We combine the converted ASR output with the diarization output in the align function. For each word in the transcript, we check which diarization segment overlaps most with that word in time (majority vote), and that speaker label is assigned to the word. We also assign a single main speaker to each ASR segment by choosing the speaker who spoke for the longest total word duration within that segment. \n",
    "\n",
    "The merge itself is straightforward (and similar to what WhisperX performs): each word is assigned to the single diarization segment that overlaps with it the most. This shows how merging can be done manually, and the align step can easily be adapted to different needs. For example, you could explicitly preserve overlaps (and maybe add per-word probabilities), merge adjacent segments with the same speaker into longer turns for cleaner transcripts, or change the voting rule (e.g., duration vs. word count). Thanks to word-level timestamps, any errors from overlaps are confined to very short spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca2c4b-28cb-4d79-8b83-6c1adb7de41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(asr_result, diarization_result):\n",
    "\n",
    "    def best_speaker(s, e):\n",
    "        best, best_ov = None, 0.0\n",
    "        for turn, _, spk in diarization_result.itertracks(yield_label=True):\n",
    "            ts, te = float(turn.start), float(turn.end)\n",
    "            ov = min(e, te) - max(s, ts) # Overlap = time where the word/segment and this diarization turn both exist\n",
    "            if ov > best_ov:\n",
    "                best, best_ov = spk, ov\n",
    "        return best\n",
    "\n",
    "    # Define how final_result shall look like (structure for saving)\n",
    "    out = {\"segments\": []}\n",
    "\n",
    "    # Loop over each ASR segment\n",
    "    for seg in asr_result[\"segments\"]:\n",
    "        \n",
    "        # 1. Add basic info: start/end time and full text\n",
    "        new_seg = {\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"text\": seg[\"text\"],\n",
    "        }\n",
    "\n",
    "        # 2. Add word-level results in a list (if word_timestamps=True)\n",
    "        words = seg.get(\"words\", [])\n",
    "        if words:\n",
    "            new_words = []\n",
    "            dur_by_speaker = {}  # Keep track of how long each speaker spoke in this segment\n",
    "            for w in words:\n",
    "                # Find which speaker overlaps most with this word\n",
    "                spk = best_speaker(w[\"start\"], w[\"end\"]) # Function from above\n",
    "                # Copy word info and add the speaker label\n",
    "                wd = dict(w)\n",
    "                wd[\"speaker\"] = spk\n",
    "                new_words.append(wd)\n",
    "                # Count how much speaking time this speaker had (duration of word)\n",
    "                if spk is not None:\n",
    "                    dur_by_speaker[spk] = dur_by_speaker.get(spk, 0.0) + (w[\"end\"] - w[\"start\"])\n",
    "            # Save the word list inside the segment\n",
    "            new_seg[\"words\"] = new_words\n",
    "            # Assign the segment-level speaker by picking the one with the most word duration\n",
    "            new_seg[\"speaker\"] = max(dur_by_speaker, key=dur_by_speaker.get) if dur_by_speaker else None\n",
    "        else:\n",
    "            # If no word-level timestamps exist, assign a speaker for the entire segment directly\n",
    "            new_seg[\"speaker\"] = best_speaker(seg[\"start\"], seg[\"end\"])\n",
    "\n",
    "        # Add this processed segment to the output\n",
    "        out[\"segments\"].append(new_seg)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f050459-d392-4d89-8510-a64f6d7c1fcf",
   "metadata": {},
   "source": [
    "Run aligner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a51ad-c09e-476a-bcd0-be0283095bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = align(asr_result, diarization_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e97df-8c2d-4aa6-a0fe-073ce9f9cfe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265cf52-c955-4384-8782-1e7af40fd84f",
   "metadata": {},
   "source": [
    "# 8. Save Final Result\n",
    "\n",
    "Finally, we save a readable transcript as a text file: for each segment, we write out the speaker label, the transcribed text, and the segmentâ€™s start time. This produces a simple \"Speaker: text [time]\" format. This can easily be adapted depending on what is needed for further analysis inside Python or in external tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab62bb-cd1b-420e-bb9c-ab9f7f38e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as ...\n",
    "method = \"fw-pyannote\"\n",
    "file = \"Buffy\"\n",
    "output_folder = \"../Results/\"\n",
    "output_name = file + \"_\" + method\n",
    "txt_path = os.path.join(output_folder, f\"{output_name}.txt\")\n",
    "\n",
    "# Save \n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for seg in final_result[\"segments\"]:\n",
    "        speaker = seg.get(\"speaker\")  # already present in your example\n",
    "        text = seg[\"text\"].strip()\n",
    "        start = str(timedelta(seconds=seg[\"start\"]))[:-3]\n",
    "        f.write(f\"{speaker}: {text} [{start}]\\n\\n\")\n",
    "\n",
    "print(\"Saved transcript to\", txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca51c83-b782-4027-8c2a-d0d42f4a6242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
